---
layout: post
title: Quick survey of recent deep learning
lecture: S3-deepNNSurvey
lectureVersion: current
extraContent:  
video: <a href="https://youtu.be/tjIJVfwSrgs"> M1</a> + <a href="https://youtu.be/Z2Pq6QokCco"> M2</a> + <a href="https://youtu.be/l58pIDazkQU"> M3</a> + <a href="https://youtu.be/gjXxOVly83s">M4</a> 
notes: <a href="https://github.com/afshinea/stanford-cs-230-deep-learning"> DNN Cheatsheets </a> 
categories: survey
tags:
- 3Classification
- Nonlinear
- Deep
- Discriminative
- 4Unsupervised
- Generative
---


This lecture covers 10 deep learning trends that go beyond classic machine learning:

 

- 0. Popular CNN, RNN, Transformer models are not covered much here

- 1. DNN on graphs / trees / sets

- 2. NTM 4program induction

- 3. Deep Generative models/ DeepFake

- 4. Deep reinforcement learning

- 5 . Few-shots / Meta learning / AGI?

- 6. pretraining workflow / Autoencoder / self-supervised training

- 7. Generative Adversarial Networks (GAN) workflow

- 8. AutoML workflow / Learning to optimize /to search architecture

- 9. Validate / Evade / Test / Verify / Understand DNNs

- 10. Model Compression / Efficient Net

 

Disclaimer: it is quite hard to make important topics of deep learning fit on a one-session schedule. We aim to make the content reasonably digestible in an introductory manner. We try to focus on a modularity view by introducing important variables to digest deep learning into chunks regarding data/ model architecture /tasks / training workflows and model characteristics. We think this teaching style provides students with context concerning those choices and helps them build a much deeper understanding.

 