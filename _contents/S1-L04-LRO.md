---
layout: post
title: GD and SGD for LR
lecture: S1-LROptimization
lectureVersion: current
extraContent:
morenotes: <a href="https://arxiv.org/abs/1609.04747"> more SGD </a> + <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf"> LR more </a> + <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">ELS Ch3.2</a>  
notes: <a href="https://nbviewer.jupyter.org/github/dtnewman/gradient_descent/blob/master/stochastic_gradient_descent.ipynb"> 
 SGD Jupyter notebook </a> + <a href="https://numpy.org/doc/stable/reference/routines.linalg.html"> numpy linalg </a> 
video: <a href="https://youtu.be/QkZIpjvtQic"> M1 </a> + <a href="https://youtu.be/7hDLwyRAjIQ">M2</a>
categories: tabular
tags:
- 2Regression
- Optimization
---
